{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4pJohDkIbwu",
    "tags": [
     "T"
    ]
   },
   "source": [
    "> **Student Names and IDs**:\n",
    ">\n",
    "> Yiteng Lu (2488152)\n",
    "> Wenge Xie(2466824)\n",
    "> Zengtian Deng (2207324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93F4Xn6mIbww",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aymIv7MFIbwz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Homework Submission Workflow\n",
    "\n",
    "When you submit your work, follow the instructions on the [submission workflow page](https://www.cs.duke.edu/courses/fall18/compsci371d/homework/workflow.html) for full credit, but see the changes mentioned below.\n",
    "\n",
    "**Important: Failure to do any of the following will result in lost points:**\n",
    "\n",
    "- Submit **one** PDF file and **one** notebook per group\n",
    "\n",
    "- Enter **all the group members** through the Gradescope GUI when you submit your PDF files. It is **not** enough to list group members in your documents\n",
    "\n",
    "- Match each **answer** (not question!) with the appropriate page in Gradescope\n",
    "\n",
    "- Avoid large blank spaces in your PDF file\n",
    "\n",
    "**Important changes to homework preparation workflow:** _This assignment is different from the others in that you are required to run it on the Google Colaboratory, a cloud service that Google makes available for reseach in machine learning. This is necessary because some of the problems require you to train a deep network on hardware that is faster than what is typically available on a standard laptop or desktop, including a high-end GPU. Even if you do have a high-end GPU, please run your notebook on the Colaboratory, so we can grade your work consistently._\n",
    "\n",
    "_**To work on this assignment, go to the [Colaboratory](https://colab.research.google.com) and upload the template notebook for this assignment through the `File` menu at the top of the Colaboratory page. Then work on the assignment, making sure to pay attention to instructions in Part 4 where you are asked to change the runtime type.**_\n",
    "\n",
    "_**When you are done, download the notebook (after making sure that all the outputs from running the code show up properly), and proceed as usual to turn that notebook into a PDF file for submission.**_\n",
    "\n",
    "#### Programming Notes\n",
    "\n",
    "+ The Colaboratory is a cloud service. If a notebook sits idle for a long time, it automatically disconnects from its execution kernel, and you need to rerun all the cells.\n",
    "+ Some of the cells in the Part on neural networks are to be run with different runtime types, as explained later. Because of this, you will not be able to just restart the notebook and run all its cells with a single command. Instead, you need to run the cells one at a time, changing runtime type as instructed. Make sure you do this once you are done with the assignment, making sure that the output from your code matches the text where you describe that output.\n",
    "+ Depending on circumstances, changing the runtime type may erase some or all of the notebook state. This will require you to rerun the cells that generate state.\n",
    "+ Training depends on random initialization of the network parameters. Because of this, your results may vary relative to the sample solution, even if your code is no different. Results may also vary from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIp6dzlwIbw0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 1: Exam-Style Questions, Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6o90sGDiK4aF",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The small neural net in the figure below uses the ReLU as the nonlinearity at the output of each neuron. The values specified in the hollow circles are biases, and the values along the edges are gains. Weigths number 1, 2, 3 refer to the first neuron, 4, 5,6 to the second, 7, 8, 9 to the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CakDYmy0Mk13",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "![a simple neural network](https://www2.cs.duke.edu/courses/spring19/compsci527/homework/5/netSimple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdrtWLQ5JZ7q",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CacoODWaJdmL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Are all the layers in the net above fully connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ9w1-GpN1FM",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o_0Co16OJ-v",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmVrDJydN7OP",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the output $y$ from the net above when the input is as follows?\n",
    "\n",
    "$$\n",
    "x_1 = 0 \\;\\;\\; \\text{and}\\;\\;\\; x_2 = 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-LZGjFMORa8",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP4qW1wUQapl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WZXG8wNQeJ5",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the gradient $\\mathbf{g}$ of the output $y$ of the network above with respect to the weight vector\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = [w_1,\\ w_2,\\ w_3,\\ w_4,\\ w_5,\\ w_6,\\ w_7,\\ w_8,\\ w_9]^T\n",
    "$$\n",
    "\n",
    "when the input has the values given in the previous problem? Just give the result if you are confident of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fVXbN93RDq7",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$\\mathbf{w} = [0,0,0,0,3,1,0,3,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_-dO56ledsy",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 2: Exam-Style Questions, Set 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWV3624YZjhu",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Let $\\mathbf{p} = f(\\mathbf{x})$ be the output of the network's soft-max layer of some neural network classifier with $K$ layers when the network's input is $\\mathbf{x}$. The classifier's output is then\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max \\mathbf{p}\\;.\n",
    "$$\n",
    "\n",
    "If $y_n$ is the true label corresponding to training input $\\mathbf{x}_n$, the loss is $\\ell_n = \\ell(y_n, f(\\mathbf{x}_n))$ for some appropriate loss function $\\ell(y, \\mathbf{p})$.\n",
    "\n",
    "We saw in class that if $\\mathbf{x}^{(k)}$ is the output from layer $k$ and $\\mathbf{w}^{(k)}$ is a vector with all the parameters in layer $k$, then back-propagation computes the partial derivatives by the following recursion,  where $\\mathbf{x}^{(0)} = \\mathbf{x}$ is the input to the network and $\\mathbf{x}^{(K)} = \\mathbf{p}$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 \\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} &=& \\frac{\\partial \\ell}{\\partial \\mathbf{p}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The derivatives above are computed for the $n$-th training sample $(\\mathbf{x}_n, y_n)$ and for the values of $\\mathbf{w}^{(k)}$ that are current at any given point during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNK7pnfDesaH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY-vJZ_Peu9c",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suppose that the network has only fully-connected layers (with ReLU nonlinearities) before the soft-max. Refer in detail to the equations given above to explain clearly why training would not work if the parameter vector $\\mathbf{w} = [\\mathbf{w}^{(1)},\\ldots, \\mathbf{w}^{(K)}]^T$ is initialized with zeros for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5CVu5NXhUpK",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "Since every $\\mathbf{w}$ is initialized to 0, every layer would have 0 zeros for output, which means the each layer always have the same ouput, making the gradient of $\\cfrac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}} $ the same and equal to 0 as well, thus $\\frac{\\partial \\mathcal{l_n}}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
    "\\;\\;\\;\\text{for} \\;\\;\\; k = K,\\ldots, 1$ equals 0 , then the weights will not update.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Vx-msEHjfdn",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJnaTzLfjiGt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A neural net classifier with only fully-connected layers (with ReLU nonlinearities) and a soft-max layer at its output has parameter vector $\\mathbf{w}$, and the network implements the function $f(\\mathbf{x}, \\mathbf{w})$ for any network input $\\mathbf{x}$. Is $\\mathbf{w} = \\mathbf{0}$ a stationary point for the function $\\phi(\\mathbf{w}) = f(\\mathbf{x}, \\mathbf{w})$ when $\\mathbf{x}$ is fixed? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxjAkNkzkDHo",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "Yes. \n",
    "Assume $ \\tilde{\\mathbf{x}} = [\\mathbf{x} ,1 ]$\n",
    "<br>\n",
    "$f(\\mathbf{x},\\mathbf{w}) =\\text{softmax}(\\mathbf{w}^{K}max({\\mathbf{w}^{K-1}} ...max(\\mathbf{w}^1\\mathbf{x},0),0),0)$ \n",
    "since $\\mathbf{w}  = \\bf{0} $\n",
    "<br>\n",
    "Any partial deraviative with respect to $\\bf{w}$ would be multiple zeros as stated in the first problem. \n",
    "<br>\n",
    "Therefore, $\\phi^{'}{(\\bf{w})} = \\bf{0}$ \n",
    ",$\\bf{w= 0} $ is a stationary point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rla5OsKGk1zi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE3Uoh6Hk5jp",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Stochastic gradient descent with momentum is used to train a certain neural network with $m$ parameters. Just before iteration $t$ of training is performed, the parameter vector has value $\\mathbf{w}_t$, and the velocity (or step) is $\\mathbf{v}_t = \\mathbf{a}$, where $\\mathbf{a}$ is some nonzero vector in $\\mathbb{R}^m$ (refer to the class notes for notation). The momentum coefficient is kept constant at $\\mu = 0.9$ throughout training. If the risk function has a saddle point at $\\mathbf{w}_t$, what is the step $\\mathbf{v}_{t+1}$ at iteration $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTU6DAPRn1hN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$\\mathbf{v}_{t+1} =0.9\\mathbf{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwMq7SK4nFwU",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEaE7twinHyl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A friend of yours argues that in the situation described in the previous problem, the steps after iteration $t$ decay exponentially. Her argument is based on the fact that the risk is at a saddle point at $\\mathbf{w}_t$, and the momentum coefficient is constant, so that $\\mathbf{v}_{t+\\tau} = \\mu^{\\tau}\\mathbf{a}$, an exponential decay. Explain why your friend's argument is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mA05ns5Unt_6",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "Because we use stochastic gradient descent, even if the weights are stucked in a saddle point which means $\\mathbf{w}_{t+1}$ will get updated, therefore it will not stuck in the saddle point all the time, which means, after few iterations, the gradient of the function will not be 0 any more. This will bring an ectra term into the $\\mathbf{v}$ equation, and make it to be $\\mathbf{v}_{t+1} = \\mu a - \\triangledown\\frac{\\partial \\mathcal{l}}{\\partial\\mathbf{w}}$. So, it will not decay exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfRyRIt3qCoz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWfz9zLXqFAV",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "In the situation described in Problem 2.3, will the training algorithm always eventually converge back towards $\\mathbf{w}_t$? Explain your answer briefly and clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv-spO4mqZcT",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "It will not always get the saddle point since we are using the stochastic gradient descent, it may go back to saddle point depends on the real-time gradient it sits on after it passes the saddle point at $t+1$  but it will not always converge back to $\\bf{w}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AMarkOTyVt3",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 3: Exam-Style Questions, Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74EwMXGkyYgz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The following problems take you through the computation of the set of all least-squares solutions to the following linear system:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "3x + 4y &=& 2\\\\\n",
    "3x + 4y &=& 3\n",
    "\\end{eqnarray*}\n",
    "\n",
    "and the solutions to a related optimization problem.\n",
    "All the answers to the questions in this problem are numerical and exact. They refer only to the data given in the problem, and no more general answers are required. You may leave your answers in the form of fractions, with expressions like the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\sqrt{3}}{2} \\left[\\begin{array}{c} 2\\\\-5\\end{array}\\right]\\;,\n",
    "$$\n",
    "\n",
    "but please simplify as much as possible.\n",
    "\n",
    "_As usual, it is easiest to answer these questions using software (and perhaps guess the exact values from the approximate ones output by your code). However, this would rob you of the opportunity to understand this material and to practice for the exam. **In any event, no answers will be accepted to problems in this part that embed software in your submission.**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac8j8SGdzDr9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9R-zasYizH16",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What are $A$ and $\\mathbf{b}$ if we write the system in this problem in the following form?\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6jJC2xuzKS5",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$$\\bf{A} = \\begin{bmatrix} 3 \\; \\; 4  \\\\ 3 \\; \\; 4 \\end{bmatrix} \\; \\; \\bf{b} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJoEvX1bzPDh",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zA4CavTxzReO",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the rank of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aplEQX65zUHW",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOrmLSUVzWM6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUH5dslBzapt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{r}$ that spans the row space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7vpRTgKzfEi",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$\\bf{r} = \\frac{1}{5}\\begin{bmatrix}3 \\\\ 4\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbW0YgUWzjQc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6kAPv09zlWk",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{n}$ that spans the null space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23yZJ6GCzo2K",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$\\bf{r} = \\frac{1}{5}\\begin{bmatrix}-4 \\\\ 3\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOm2a0USzwU0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMNnKIARzyfL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Write the matrix $V$ in the SVD $A = U\\Sigma V^T$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7U8WqRUz0ct",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$V = \\begin{bmatrix} \\frac{3}{5}  \\; -\\frac{4} \n",
    "{5} \\\\\n",
    "\\frac{4}{5}  \\; \\;\\;\\;\\; \\frac{3}{5} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zk41d7sBz5GS",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZK3cYMIkz7Jc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the matrices $U$ and $\\Sigma$ in the SVD of $A$. [Hint: compute $U\\Sigma$ first.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3ZNlDaUz9gQ",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$U = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\; -1 \\\\ 1 \\; \\; \\;\\; 1\\end{bmatrix},    \\Sigma =  \\begin{bmatrix} \\sqrt{50} \\;  \\; \\; 0 \\\\ \\; \\; \\; 0 \\;  \\; \\; \\; \\;0  \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHCt-FvH0FDT",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhxCKF6U0HwH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the pseudo-inverse $A^{\\dagger}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63rXWdST0KZN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBzmlRmoUN2h"
   },
   "source": [
    "$A^{\\dagger} =\\frac{1}{50} \\begin{bmatrix} 3 \\; 3 \\\\4 \\;  4\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hllY12AA0PKi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnjLAWvt0Q06",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find the minimum-norm least-squares solution $\\mathbf{x}^*$ of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RgECzBj0TVw",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$\\bf{x}^*=\\frac{1}{10}\\begin{bmatrix}3 \\\\ 4 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDz6wROp0ZIr",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P5mlHPz0bTA",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give an expression for the set $S$ of all least-squares solutions of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2ZKErfi0dwK",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer\n",
    "$ S = \\frac{1}{10}\\begin{bmatrix}3 \\\\ 4 \\end{bmatrix} + \\alpha\\begin{bmatrix}-4 \\\\ 3\\end{bmatrix}$ for $\\alpha \\in R$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vQg6So00ij9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghlx7z5s0lkG",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find all the solutions to\n",
    "\n",
    "$$\n",
    " \\hat{\\mathbf{x}} = \\arg\\min_{\\|\\mathbf{x}\\| = 1} \\|A\\mathbf{x}\\|\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXELtk700oFd",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTt51Ye7b9tR"
   },
   "source": [
    "$\\hat{\\bf{x}} = \\alpha \\begin{bmatrix} -4 \\\\ 3 \\end{bmatrix}$ for $\\alpha \\in \\begin{Bmatrix}-\\frac{1}{5}, \\frac{1}{5}\\end{Bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GZdQ7IsoUVR",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gQIvWQAzhzm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The code in this part is somewhat modified from the [Keras documentation](https://keras.io/examples/cifar10_cnn/). It downloads the CIFAR-10 dataset, a set of 60000 labeled images grouped in 10 categories, which it splits into training, validation, and test sets. It then defines a function `network` that returns a simple convolutional neural network (the `model`), and a function `train` that trains the model for a single epoch by default, checking performance on the validation set.The function `train` also saves the trained model in a file in the cloud and evaluates the model on the test data. Finally, it returns a history of training and validation accuracies achieved after each epoch of training. The function `train` uses SGD as the default optimizer.\n",
    "\n",
    "_**Important:**_ Make sure you select Python 3 through the `Runtime->Change runtime type` menu at the top of the notebook. Also set the hardware acceleration to `None` in that same menu. We will turn on GPU acceleration later on. TPU acceleration is not always available, so we won't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "JM7Sh_kizkO-",
    "outputId": "1194a7bc-2e4b-4868-e2d2-75e06b647b65",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 46s 0us/step\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "40000 training samples\n",
      "10000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, split between train, validation, and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
    "                                                             test_size=0.2)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print(x_validate.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_validate = x_validate.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_validate /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "FBMRAFNN5k6S",
    "outputId": "a408a8c4-34e6-420b-b818-a7cc46e914ac",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "activation_function = 'relu'\n",
    "\n",
    "def network(activation_function='relu'):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                  input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35sd9xJr55eM",
    "tags": [
     "HST"
    ]
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "def train(model, epochs=1,\n",
    "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
    "          verbose=2):\n",
    "\n",
    "  batch_size = 32\n",
    "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "  model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "  # Configure the model for training\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_validate, y_validate),\n",
    "                      shuffle=True,\n",
    "                      verbose=verbose)\n",
    "\n",
    "  # Save model and weights\n",
    "  if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "  model_path = os.path.join(save_dir, model_name)\n",
    "  model.save(model_path)\n",
    "\n",
    "  # Score trained model.\n",
    "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "  print('Test loss:', scores[0])\n",
    "  print('Test accuracy:', scores[1])\n",
    "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMElXg2ETjAE",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDnPx1gdT5Jm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Using Stochastic Gradient Descent (SGD) with the default parameters in `train`, train the model for one epoch _with no hardware acceleration_.\n",
    "\n",
    "Show your call to `train` and the outputs it generates. Is validation accuracy a reasonably good estimate of test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73TQApgKiGY_",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Hardware acceleration is turned off through the `Runtime->Change runtime type` menu at the top of the notebook, and selecting `None` for hardware acceleration.\n",
    "\n",
    "+ Depending on circumstances, after you change the runtime type, some or all of the notebook state may be lost. This will require you to rerun some of the cells above.\n",
    "\n",
    "+ Tensorflow may generate warning messages that depend on how the Colaboratory interface is implemented. These messages are typically harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sYqNFyUVxdy",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "OsQfvsx6gMKV",
    "outputId": "45573555-4762-4480-a246-c99c42b391c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 239s - loss: 1.5087 - acc: 0.4532 - val_loss: 1.3209 - val_acc: 0.5228\n",
      "10000/10000 [==============================] - 17s 2ms/step\n",
      "Test loss: 1.3148798112869262\n",
      "Test accuracy: 0.5279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.453225], [0.5228]]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKW8ABAzTQHM"
   },
   "source": [
    "The validation accuracy can be an esitmate of the test accuracy because the discrepency between them is only 0.0051, which is a very small difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoGPF3W7WU_X",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCDoSETDWXCJ",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Repeat the previous experiment _after turning on GPU acceleration_ from the `Runtime->Change runtime type` menu.\n",
    "\n",
    "Are the accuracy values the same as before? Explain why or why not. What is the approximate ratio of running times of CPU (no acceleration) versus GPU training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRnVqg5gXbvx",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "9nZltxNSixsZ",
    "outputId": "5a99a2a1-9c14-40c8-dda6-7200016c9673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 13s - loss: 1.4208 - acc: 0.4838 - val_loss: 1.3319 - val_acc: 0.5222\n",
      "10000/10000 [==============================] - 1s 123us/step\n",
      "Test loss: 1.318236988067627\n",
      "Test accuracy: 0.5271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.48385], [0.5222]]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRJCRdfccq-x"
   },
   "source": [
    "Yes, it is the same. GPU just computes fast but it only trains the network once on one epoch, given all the hyperparameters are the same setting as preivous experiment on CPU, the result should be very close.\n",
    "<br>\n",
    "The ratio of running times on cpu versus running times on GPU is around 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL293H0_YerN",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw67mTPzYg-4",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We keep GPU acceleration turned on from now on.\n",
    "\n",
    "Repeat the experiment above with the ADAM optimizer with the default parameters. This optimizer selects the descent step size adaptively. The ADAM optimizer is invoked by using parameter `opt = keras.optimizers.Adam()` in `train`.\n",
    "\n",
    "Compare accuracies and running times with those achieved in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRFT6uhiZdIj",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "kVrjOsBcjc0C",
    "outputId": "bbf9309a-0b2f-40ac-be16-00286fda0b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 16s - loss: 1.0614 - acc: 0.6245 - val_loss: 0.8898 - val_acc: 0.6895\n",
      "10000/10000 [==============================] - 1s 126us/step\n",
      "Test loss: 0.901596240234375\n",
      "Test accuracy: 0.6853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.624475], [0.6895]]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, opt=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzK9EeOncRRW"
   },
   "source": [
    "It has much higher accuracies than the preivous runnings both on testing and validations set. The running time is close to the experiment in question 4.2, much faster than that in question 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEp7_h36bC7R",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdkfA9ibFUM",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We use the ADAM optimizer with default parameters from now on.\n",
    "\n",
    "Repeat the previous experiment with 30 epochs of training instead of 1 (`epochs=30`). This time, store the value returned by `train`, as you will need it for plotting.\n",
    "\n",
    "When done, plot both training accuracy and validation accuracy as functions of epoch number on the same diagram. Label the axes and add a legend to specify which plot is which.\n",
    "\n",
    "Do you think that the classifier would perform much better if you were to train longer? Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw9glyaNvwQ6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Look at the definition of `train` to figure out what the output from that function contains.\n",
    "+ Set the value of the `verbose` parameter in the call to `train` to 0 to suppress output, which would be too long to include in your PDF file. You can estimate from your previous experiments how long the code will take to run. Alternatively, set `verbose` to 2 in early test runs, but then set it to 0 in your final run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq5m18PJlc7_",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "1x9Kr4Czj739",
    "outputId": "9d965edb-ea47-4780-b16b-ec637fb13a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 1s 130us/step\n",
      "Test loss: 0.6946677005290985\n",
      "Test accuracy: 0.7786\n"
     ]
    }
   ],
   "source": [
    "A=[]\n",
    "A = train(model, epochs =30 , opt=keras.optimizers.Adam(),verbose = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "_xAAAE-Tmbvy",
    "outputId": "fb05176c-36bd-4707-8366-857361fe2cb6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAlCHQCAMPH/eVWrBYt89tkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(A[1])\n",
    "plt.plot(A[2])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGHnFecFwz_8"
   },
   "source": [
    "The classifier will not perform much better if I set it to train a longer time. Training on the same dataset for a really long time may gain a really small training error. However, this is not good when testing. The network tends to overfit if we overly trained it. So, although a low training error can be gained from long time training, the performance of this classifier may actually decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdW8dV4uoCq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A97WOgV8uweq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suggest at least three different ways to improve the performance of the classifier defined in this Part. For each way, explain why that would help. This is an open-ended question, and answers may vary. Do _not_ implement your suggestions, and do _not_ refer to techniques we have not covered in class (such as batch normalization or other techniques you may have heard of).\n",
    "\n",
    "If you suggest more than three ways, we will grade you for the best ones. However, we _will_ deduct points for patently wrong statements in any of your suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12XxnmJwnYzN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPOmEFTbxbAu"
   },
   "source": [
    "First, we can enlarge the size of the training set. Using more data to train . the model can effectively reduce overfitting, which leads to a better performace of this classifier.\n",
    "Second, we can choose a better learning rate by cross-validation. Learning rate is a parameter of critical importance. a rate that is too large leads to large steps that often overshoot, and a rate that is too small leads to very slow progress. So, choosing the learning rate properly is also a way to improve performance.\n",
    "Third, we can choose a good epoch by monitoring the validation error on the validation set. Terminating the algorithm at the point where the validation error starts to increase can prevent overfitting too. By this way, we also can improve the performance of the classifier.\n",
    "Fourth, choosing a proper batch size can also be helpful. It turns out that small values of batch size achieve the best compromise between reducing variance and keeping steps flexible. We can also choose a proper batch size by cross-validation to improve the performance of the classifier.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of homework05.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
