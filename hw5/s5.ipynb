{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93F4Xn6mIbww",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejMnSWJnIbwx",
    "tags": [
     "S"
    ]
   },
   "source": [
    "# Solution Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aymIv7MFIbwz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Homework Submission Workflow\n",
    "\n",
    "When you submit your work, follow the instructions on the [submission workflow page](https://www.cs.duke.edu/courses/fall18/compsci371d/homework/workflow.html) for full credit, but see the changes mentioned below.\n",
    "\n",
    "**Important: Failure to do any of the following will result in lost points:**\n",
    "\n",
    "- Submit **one** PDF file and **one** notebook per group\n",
    "\n",
    "- Enter **all the group members** through the Gradescope GUI when you submit your PDF files. It is **not** enough to list group members in your documents\n",
    "\n",
    "- Match each **answer** (not question!) with the appropriate page in Gradescope\n",
    "\n",
    "- Avoid large blank spaces in your PDF file\n",
    "\n",
    "**Important changes to homework preparation workflow:** _This assignment is different from the others in that you are required to run it on the Google Colaboratory, a cloud service that Google makes available for reseach in machine learning. This is necessary because some of the problems require you to train a deep network on hardware that is faster than what is typically available on a standard laptop or desktop, including a high-end GPU. Even if you do have a high-end GPU, please run your notebook on the Colaboratory, so we can grade your work consistently._\n",
    "\n",
    "_**To work on this assignment, go to the [Colaboratory](https://colab.research.google.com) and upload the template notebook for this assignment through the `File` menu at the top of the Colaboratory page. Then work on the assignment, making sure to pay attention to instructions in Part 4 where you are asked to change the runtime type.**_\n",
    "\n",
    "_**When you are done, download the notebook (after making sure that all the outputs from running the code show up properly), and proceed as usual to turn that notebook into a PDF file for submission.**_\n",
    "\n",
    "#### Programming Notes\n",
    "\n",
    "+ The Colaboratory is a cloud service. If a notebook sits idle for a long time, it automatically disconnects from its execution kernel, and you need to rerun all the cells.\n",
    "+ Some of the cells in the Part on neural networks are to be run with different runtime types, as explained later. Because of this, you will not be able to just restart the notebook and run all its cells with a single command. Instead, you need to run the cells one at a time, changing runtime type as instructed. Make sure you do this once you are done with the assignment, making sure that the output from your code matches the text where you describe that output.\n",
    "+ Depending on circumstances, changing the runtime type may erase some or all of the notebook state. This will require you to rerun the cells that generate state.\n",
    "+ Training depends on random initialization of the network parameters. Because of this, your results may vary relative to the sample solution, even if your code is no different. Results may also vary from run to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIp6dzlwIbw0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 1: Exam-Style Questions, Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6o90sGDiK4aF",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The small neural net in the figure below uses the ReLU as the nonlinearity at the output of each neuron. The values specified in the hollow circles are biases, and the values along the edges are gains. Weigths number 1, 2, 3 refer to the first neuron, 4, 5,6 to the second, 7, 8, 9 to the third."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CakDYmy0Mk13",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "![a simple neural network](https://www2.cs.duke.edu/courses/spring19/compsci527/homework/5/netSimple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdrtWLQ5JZ7q",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CacoODWaJdmL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Are all the layers in the net above fully connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ9w1-GpN1FM",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU_-6twfN2r1",
    "tags": [
     "S"
    ]
   },
   "source": [
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o_0Co16OJ-v",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmVrDJydN7OP",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the output $y$ from the net above when the input is as follows?\n",
    "\n",
    "$$\n",
    "x_1 = 0 \\;\\;\\; \\text{and}\\;\\;\\; x_2 = 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-LZGjFMORa8",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysZQ9VfeOS5h",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "y = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP4qW1wUQapl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WZXG8wNQeJ5",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the gradient $\\mathbf{g}$ of the output $y$ of the network above with respect to the weight vector\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = [w_1,\\ w_2,\\ w_3,\\ w_4,\\ w_5,\\ w_6,\\ w_7,\\ w_8,\\ w_9]^T\n",
    "$$\n",
    "\n",
    "when the input has the values given in the previous problem? Just give the result if you are confident of your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fVXbN93RDq7",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtduD0H0RGXW",
    "tags": [
     "S"
    ]
   },
   "source": [
    "Activations can be computed in a forward pass (from the previous problem) to be\n",
    "\n",
    "$$\n",
    "a_1 = 0 \\;\\;\\;,\\;\\;\\; a_2 = 3 \\;\\;\\;,\\;\\;\\; y = a_3 = 4\\;.\n",
    "$$\n",
    "\n",
    "With these values, we have\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y &=& w_7 a_1 + w_8 a_2 + w_9 \\\\\n",
    "a_1 &=& 0 \\\\\n",
    "a_2 &=& w_4 x_1 + w_5 x_2 + w_6\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Letting $g_k = \\partial y/\\partial w_k$, a backward pass yields the entries of $\\mathbf{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lclcl}\n",
    "g_7 = a_1 = 0 &,& g_8 = a_2 = 3 &,& g_9 = 1 \\\\\n",
    "g_1 = 0 &,& g_2 = 0  &,& g_3 =  0 \\\\\n",
    "g_4 = w_8 x_1 = 0 &,& g_5 = w_8 x_2 = 3 &,& g_6 = w_8 = 1 \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\mathbf{g} = [0, 0, 0, 0, 3, 1, 0, 3, 1] ^T\\;.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_-dO56ledsy",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "\n",
    "## Part 2: Exam-Style Questions, Set 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWV3624YZjhu",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Let $\\mathbf{p} = f(\\mathbf{x})$ be the output of the network's soft-max layer of some neural network classifier with $K$ layers when the network's input is $\\mathbf{x}$. The classifier's output is then\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max \\mathbf{p}\\;.\n",
    "$$\n",
    "\n",
    "If $y_n$ is the true label corresponding to training input $\\mathbf{x}_n$, the loss is $\\ell_n = \\ell(y_n, f(\\mathbf{x}_n))$ for some appropriate loss function $\\ell(y, \\mathbf{p})$.\n",
    "\n",
    "We saw in class that if $\\mathbf{x}^{(k)}$ is the output from layer $k$ and $\\mathbf{w}^{(k)}$ is a vector with all the parameters in layer $k$, then back-propagation computes the partial derivatives by the following recursion,  where $\\mathbf{x}^{(0)} = \\mathbf{x}$ is the input to the network and $\\mathbf{x}^{(K)} = \\mathbf{p}$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{w}^{(k)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 1 \\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}} &=& \\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k)}} \\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}}\n",
    "\\;\\;\\;\\text{for}\\;\\;\\; k = K,\\ldots, 2\\\\\n",
    "\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(K)}} &=& \\frac{\\partial \\ell}{\\partial \\mathbf{p}}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The derivatives above are computed for the $n$-th training sample $(\\mathbf{x}_n, y_n)$ and for the values of $\\mathbf{w}^{(k)}$ that are current at any given point during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNK7pnfDesaH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY-vJZ_Peu9c",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suppose that the network has only fully-connected layers (with ReLU nonlinearities) before the soft-max. Refer in detail to the equations given above to explain clearly why training would not work if the parameter vector $\\mathbf{w} = [\\mathbf{w}^{(1)},\\ldots, \\mathbf{w}^{(K)}]^T$ is initialized with zeros for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5CVu5NXhUpK",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19Qi_imshY2c",
    "tags": [
     "S"
    ]
   },
   "source": [
    "If $V^{(k)}$ is the matrix of gains in layer $k$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{x}^{(k)}}{\\partial \\mathbf{x}^{(k-1)}} = V^{(k)}\n",
    "$$\n",
    "\n",
    "and if all the weights are initialized to zero, this Jacobian is zero. Therefore, $\\frac{\\partial \\ell_n}{\\partial \\mathbf{x}^{(k-1)}}$ is also zero for $k = 2,\\ldots, K$ (second equation above), and therefore, from the first equation, $\\frac{\\partial \\ell_n}{\\partial \\mathbf{w}^{(k)}} = 0$ for all $k$. Thus, the gradient is always zero, and no progress occurs during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Vx-msEHjfdn",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJnaTzLfjiGt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A neural net classifier with only fully-connected layers (with ReLU nonlinearities) and a soft-max layer at its output has parameter vector $\\mathbf{w}$, and the network implements the function $f(\\mathbf{x}, \\mathbf{w})$ for any network input $\\mathbf{x}$. Is $\\mathbf{w} = \\mathbf{0}$ a stationary point for the function $\\phi(\\mathbf{w}) = f(\\mathbf{x}, \\mathbf{w})$ when $\\mathbf{x}$ is fixed? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxjAkNkzkDHo",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE8-lBTxkEWL",
    "tags": [
     "S"
    ]
   },
   "source": [
    "Yes, $\\mathbf{w} = \\mathbf{0}$ is a stationary point for $\\phi(\\mathbf{w})$, because we saw in the previous problem that $\\phi$ has zero gradient there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rla5OsKGk1zi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE3Uoh6Hk5jp",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Stochastic gradient descent with momentum is used to train a certain neural network with $m$ parameters. Just before iteration $t$ of training is performed, the parameter vector has value $\\mathbf{w}_t$, and the velocity (or step) is $\\mathbf{v}_t = \\mathbf{a}$, where $\\mathbf{a}$ is some nonzero vector in $\\mathbb{R}^m$ (refer to the class notes for notation). The momentum coefficient is kept constant at $\\mu = 0.9$ throughout training. If the risk function has a saddle point at $\\mathbf{w}_t$, what is the step $\\mathbf{v}_{t+1}$ at iteration $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTU6DAPRn1hN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ivz_KL6in3Gn",
    "tags": [
     "S"
    ]
   },
   "source": [
    "The step at iteration $t$ is $\\mathbf{v}_{t+1} = 0.9\\, \\mathbf{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwMq7SK4nFwU",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEaE7twinHyl",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "A friend of yours argues that in the situation described in the previous problem, the steps after iteration $t$ decay exponentially. Her argument is based on the fact that the risk is at a saddle point at $\\mathbf{w}_t$, and the momentum coefficient is constant, so that $\\mathbf{v}_{t+\\tau} = \\mu^{\\tau}\\mathbf{a}$, an exponential decay. Explain why your friend's argument is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mA05ns5Unt_6",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAdWQ-gZnvis",
    "tags": [
     "S"
    ]
   },
   "source": [
    "While the step at iteration $t$ is $\\mathbf{v}_{t+1} = \\mu\\, \\mathbf{a}$, the subsequent step is\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{t+2} = \\mu \\mathbf{v}_{t+1}- \\alpha \\nabla L_T(\\mathbf{w}_{t+1}) \\;.\n",
    "$$\n",
    "\n",
    "Since the gradient of the risk function at $\\mathbf{w}_{t+1}$ is no longer necessarily zero, the new step is not necessarily equal to $\\mu \\mathbf{v}_{t+1} = \\mu^2 \\mathbf{a}$, as would be required for an exponential decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfRyRIt3qCoz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWfz9zLXqFAV",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "In the situation described in Problem 2.3, will the training algorithm always eventually converge back towards $\\mathbf{w}_t$? Explain your answer briefly and clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv-spO4mqZcT",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "veH3uhZtqdrw",
    "tags": [
     "S"
    ]
   },
   "source": [
    "No, training will not necessarily converge towards $\\mathbf{w}_t$. The step $\\mathbf{v}_{t+1}$ moves the algorithm away from the saddle point. The new negative gradient at $\\mathbf{w}_{t+1}$ may point away from $\\mathbf{w}_t$, and it is then possible that the algorithm eventually escapes the saddle point toward parameter vectrors with lower risk, never to return to $\\mathbf{w}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AMarkOTyVt3",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 3: Exam-Style Questions, Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74EwMXGkyYgz",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The following problems take you through the computation of the set of all least-squares solutions to the following linear system:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "3x + 4y &=& 2\\\\\n",
    "3x + 4y &=& 3\n",
    "\\end{eqnarray*}\n",
    "\n",
    "and the solutions to a related optimization problem.\n",
    "All the answers to the questions in this problem are numerical and exact. They refer only to the data given in the problem, and no more general answers are required. You may leave your answers in the form of fractions, with expressions like the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\sqrt{3}}{2} \\left[\\begin{array}{c} 2\\\\-5\\end{array}\\right]\\;,\n",
    "$$\n",
    "\n",
    "but please simplify as much as possible.\n",
    "\n",
    "_As usual, it is easiest to answer these questions using software (and perhaps guess the exact values from the approximate ones output by your code). However, this would rob you of the opportunity to understand this material and to practice for the exam. **In any event, no answers will be accepted to problems in this part that embed software in your submission.**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac8j8SGdzDr9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9R-zasYizH16",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What are $A$ and $\\mathbf{b}$ if we write the system in this problem in the following form?\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6jJC2xuzKS5",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GY022ohGzMUz",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "A = \\left[\\begin{array}{cc} 3 & 4\\\\3 & 4\\end{array}\\right] \\;\\;\\;and\\;\\;\\; \\mathbf{b} = \\left[\\begin{array}{c} 2\\\\3\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJoEvX1bzPDh",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zA4CavTxzReO",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "What is the rank of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aplEQX65zUHW",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-x8iu7gFzY2O",
    "tags": [
     "S"
    ]
   },
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOrmLSUVzWM6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUH5dslBzapt",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{r}$ that spans the row space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7vpRTgKzfEi",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvRsAx6hzhPk",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "\\mathbf{r} = \\frac{1}{5} \\left[\\begin{array}{c} 3\\\\4\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbW0YgUWzjQc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6kAPv09zlWk",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give a _unit_ column vector $\\mathbf{n}$ that spans the null space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23yZJ6GCzo2K",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmQTBQd3zrZ_",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "\\mathbf{n} = \\frac{1}{5} \\left[\\begin{array}{c} -4\\\\3\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOm2a0USzwU0",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMNnKIARzyfL",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Write the matrix $V$ in the SVD $A = U\\Sigma V^T$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7U8WqRUz0ct",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETzUQV3Zz2By",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "V = \\left[\\begin{array}{cc} \\mathbf{r} & \\mathbf{n}\\end{array}\\right] = \\frac{1}{5} \\left[\\begin{array}{cc} 3 & -4\\\\ 4 & 3\\end{array}\\right]\n",
    "$$\n",
    "(the sign of either column can be flipped for an equally acceptable answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zk41d7sBz5GS",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZK3cYMIkz7Jc",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the matrices $U$ and $\\Sigma$ in the SVD of $A$. [Hint: compute $U\\Sigma$ first.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3ZNlDaUz9gQ",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67TS1NF9z_ko",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "U\\Sigma = AV = \\frac{1}{5} \\left[\\begin{array}{cc} 3 & 4\\\\3 & 4\\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} 3 & -4\\\\ 4 & 3\\end{array}\\right] =\n",
    "\\frac{1}{5} \\left[\\begin{array}{cc} 25 & 0\\\\25 & 0\\end{array}\\right] =\n",
    "\\left[\\begin{array}{cc} 5 & 0\\\\5 & 0\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "so that the first column of $U$ is $[1, 1]^T$ normalized to unit norm:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_1 = \\frac{\\sqrt{2}}{2} \\left[\\begin{array}{c} 1\\\\1\\end{array}\\right]\\;.\n",
    "$$\n",
    "\n",
    "The second column of $U$ is orthogonal to the first and has unit norm, so\n",
    "\n",
    "$$\n",
    "U = \\frac{\\sqrt{2}}{2} \\left[\\begin{array}{cc} 1 & -1\\\\1 & 1\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "(the sign of the second column can be flipped for an equally good answer, and the sign of the first column depends on that of $\\mathbf{r}$).\n",
    "\n",
    "The matrix $\\Sigma$ is\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{cc} \\sigma & 0\\\\0 & 0\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "because $A$ is rank 1, and $\\sigma$ is $5$ divided by the normalization factor of $\\mathbf{u}_1$:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{10}{\\sqrt{2}} = 5\\sqrt{2}\\;,\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left[\\begin{array}{cc} 5\\sqrt{2} & 0\\\\0 & 0\\end{array}\\right] \\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHCt-FvH0FDT",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhxCKF6U0HwH",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Compute the pseudo-inverse $A^{\\dagger}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63rXWdST0KZN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwv7KNCt0McX",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "A^{\\dagger} = V \\Sigma^{\\dagger} U^T = \\frac{1}{5} \\left[\\begin{array}{cc} 3 & -4\\\\ 4 & 3\\end{array}\\right]\n",
    "\\frac{1}{5\\sqrt{2}} \\left[\\begin{array}{cc} 1 & 0\\\\0 & 0\\end{array}\\right] \n",
    "\\frac{\\sqrt{2}}{2} \\left[\\begin{array}{cc} 1 & 1\\\\-1 & 1\\end{array}\\right] =\n",
    "\\frac{1}{50} \\left[\\begin{array}{cc} 3 & -4\\\\ 4 & 3\\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} 1 & 1\\\\0 & 0\\end{array}\\right] \n",
    "= \\frac{1}{50} \\left[\\begin{array}{cc} 3 & 3\\\\ 4 & 4\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hllY12AA0PKi",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnjLAWvt0Q06",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find the minimum-norm least-squares solution $\\mathbf{x}^*$ of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RgECzBj0TVw",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGlsf-Px0WlW",
    "tags": [
     "S"
    ]
   },
   "source": [
    "$$\n",
    "\\mathbf{x}^* = A^{\\dagger} \\mathbf{b} = \\frac{1}{50} \\left[\\begin{array}{cc} 3 & 3\\\\ 4 & 4\\end{array}\\right]\n",
    "\\left[\\begin{array}{c} 2\\\\3\\end{array}\\right] = \\frac{1}{50} \\left[\\begin{array}{c} 15\\\\20\\end{array}\\right]\n",
    "= \\frac{1}{10} \\left[\\begin{array}{c} 3\\\\4\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDz6wROp0ZIr",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P5mlHPz0bTA",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Give an expression for the set $S$ of all least-squares solutions of the system $A\\mathbf{x} = \\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2ZKErfi0dwK",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wv9RRvY70fso",
    "tags": [
     "S"
    ]
   },
   "source": [
    "The set $S$ is the minimum-norm solution plus any vector in the null space:\n",
    "\n",
    "$$\n",
    "S = \\{\\mathbf{x}^* \\;+\\; \\alpha \\mathbf{n} \\} = \\left\\{\\left[\\begin{array}{c} 0.3\\\\0.4\\end{array}\\right] \\;+\\; \\alpha\\left[\\begin{array}{c} -0.8\\\\0.6\\end{array}\\right] \\right\\}\n",
    "= \\left\\{\\left[\\begin{array}{c} 0.3 - 0.8 \\alpha\\\\0.4 + 0.6\\alpha\\end{array}\\right]\\right\\}\n",
    "$$\n",
    "\n",
    "for any $\\alpha\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vQg6So00ij9",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghlx7z5s0lkG",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Find all the solutions to\n",
    "\n",
    "$$\n",
    " \\hat{\\mathbf{x}} = \\arg\\min_{\\|\\mathbf{x}\\| = 1} \\|A\\mathbf{x}\\|\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXELtk700oFd",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZZHRnev0rJp",
    "tags": [
     "S"
    ]
   },
   "source": [
    "There are two solutions:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\pm \\mathbf{n} = \\pm \\left[\\begin{array}{c} -0.8\\\\0.6\\end{array}\\right]\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GZdQ7IsoUVR",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "## Part 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gQIvWQAzhzm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "The code in this part is somewhat modified from the [Keras documentation](https://keras.io/examples/cifar10_cnn/). It downloads the CIFAR-10 dataset, a set of 60000 labeled images grouped in 10 categories, which it splits into training, validation, and test sets. It then defines a function `network` that returns a simple convolutional neural network (the `model`), and a function `train` that trains the model for a single epoch by default, checking performance on the validation set.The function `train` also saves the trained model in a file in the cloud and evaluates the model on the test data. Finally, it returns a history of training and validation accuracies achieved after each epoch of training. The function `train` uses SGD as the default optimizer.\n",
    "\n",
    "_**Important:**_ Make sure you select Python 3 through the `Runtime->Change runtime type` menu at the top of the notebook. Also set the hardware acceleration to `None` in that same menu. We will turn on GPU acceleration later on. TPU acceleration is not always available, so we won't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "JM7Sh_kizkO-",
    "outputId": "dceac85a-9f95-4fdf-b52c-324b77f31f90",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 31s 0us/step\n",
      "x_train shape: (40000, 32, 32, 3)\n",
      "40000 training samples\n",
      "10000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, split between train, validation, and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "(x_train, x_validate, y_train, y_validate) = train_test_split(x_train, y_train,\n",
    "                                                             test_size=0.2)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print(x_validate.shape[0], 'validation samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_validate = keras.utils.to_categorical(y_validate, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_validate = x_validate.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_validate /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "FBMRAFNN5k6S",
    "outputId": "96bc7814-80a5-4547-c462-57a68b9121cd",
    "tags": [
     "HST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "activation_function = 'relu'\n",
    "\n",
    "def network(activation_function='relu'):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                  input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation(activation_function))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35sd9xJr55eM",
    "tags": [
     "HST"
    ]
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "def train(model, epochs=1,\n",
    "          opt = keras.optimizers.SGD(lr=0.01, momentum=0.7, decay=0.001),\n",
    "          verbose=2):\n",
    "\n",
    "  batch_size = 32\n",
    "  save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "  model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "  # Configure the model for training\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_validate, y_validate),\n",
    "                      shuffle=True,\n",
    "                      verbose=verbose)\n",
    "\n",
    "  # Save model and weights\n",
    "  if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "  model_path = os.path.join(save_dir, model_name)\n",
    "  model.save(model_path)\n",
    "\n",
    "  # Score trained model.\n",
    "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "  print('Test loss:', scores[0])\n",
    "  print('Test accuracy:', scores[1])\n",
    "  return [history.epoch, history.history['acc'], history.history['val_acc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMElXg2ETjAE",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDnPx1gdT5Jm",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Using Stochastic Gradient Descent (SGD) with the default parameters in `train`, train the model for one epoch _with no hardware acceleration_.\n",
    "\n",
    "Show your call to `train` and the outputs it generates. Is validation accuracy a reasonably good estimate of test accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73TQApgKiGY_",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Hardware acceleration is turned off through the `Runtime->Change runtime type` menu at the top of the notebook, and selecting `None` for hardware acceleration.\n",
    "\n",
    "+ Depending on circumstances, after you change the runtime type, some or all of the notebook state may be lost. This will require you to rerun some of the cells above.\n",
    "\n",
    "+ Tensorflow may generate warning messages that depend on how the Colaboratory interface is implemented. These messages are typically harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sYqNFyUVxdy",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "vv_g54pfg4NT",
    "outputId": "72514e7b-99d0-4f3b-b6d6-a66e00cf9509",
    "tags": [
     "S"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 237s - loss: 2.0346 - acc: 0.2452 - val_loss: 1.7759 - val_acc: 0.3665\n",
      "10000/10000 [==============================] - 17s 2ms/step\n",
      "Test loss: 1.7724207149505615\n",
      "Test accuracy: 0.3771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.2452], [0.3665]]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SP23n79VzRx",
    "tags": [
     "S"
    ]
   },
   "source": [
    "Validation accuracy is a good estimate of test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoGPF3W7WU_X",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCDoSETDWXCJ",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Repeat the previous experiment _after turning on GPU acceleration_ from the `Runtime->Change runtime type` menu.\n",
    "\n",
    "Are the accuracy values the same as before? Explain why or why not. What is the approximate ratio of running times of CPU (no acceleration) versus GPU training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRnVqg5gXbvx",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "gwyRhIUbj_KV",
    "outputId": "b36b3585-7421-41fd-bcdf-0ff3c0ea2605",
    "tags": [
     "S"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 17s - loss: 1.9952 - acc: 0.2668 - val_loss: 1.7304 - val_acc: 0.3726\n",
      "10000/10000 [==============================] - 1s 122us/step\n",
      "Test loss: 1.7190162654876708\n",
      "Test accuracy: 0.3841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.2668], [0.3726]]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4lN4MaIXdTk",
    "tags": [
     "S"
    ]
   },
   "source": [
    "The accuracy values above are different from those achieved in the previous problem. The reason is that the model was re-initialized with random values, and these values are in general different every time.The SGD paths from different initializers may be different.\n",
    "\n",
    "The running time is now about 14 times faster than with no acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL293H0_YerN",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw67mTPzYg-4",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We keep GPU acceleration turned on from now on.\n",
    "\n",
    "Repeat the experiment above with the ADAM optimizer with the default parameters. This optimizer selects the descent step size adaptively. The ADAM optimizer is invoked by using parameter `opt = keras.optimizers.Adam()` in `train`.\n",
    "\n",
    "Compare accuracies and running times with those achieved in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRFT6uhiZdIj",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "VQ2tcSlXkrNu",
    "outputId": "c4e6c63f-bc9c-4053-8899-1f6a454547b0",
    "tags": [
     "S"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 16s - loss: 1.4922 - acc: 0.4553 - val_loss: 1.2654 - val_acc: 0.5424\n",
      "10000/10000 [==============================] - 1s 139us/step\n",
      "Test loss: 1.2508435594558716\n",
      "Test accuracy: 0.5453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0], [0.455275], [0.5424]]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, opt=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYV7MFqyZewm",
    "tags": [
     "S"
    ]
   },
   "source": [
    "The running time is about the same as for SGD. However,  accuracies after one epoch are significantly better. Of course, results may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEp7_h36bC7R",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdkfA9ibFUM",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "We use the ADAM optimizer with default parameters from now on.\n",
    "\n",
    "Repeat the previous experiment with 30 epochs of training instead of 1 (`epochs=30`). This time, store the value returned by `train`, as you will need it for plotting.\n",
    "\n",
    "When done, plot both training accuracy and validation accuracy as functions of epoch number on the same diagram. Label the axes and add a legend to specify which plot is which.\n",
    "\n",
    "Do you think that the classifier would perform much better if you were to train longer? Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw9glyaNvwQ6",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "#### Programming Notes\n",
    "\n",
    "+ Look at the definition of `train` to figure out what the output from that function contains.\n",
    "+ Set the value of the `verbose` parameter in the call to `train` to 0 to suppress output, which would be too long to include in your PDF file. You can estimate from your previous experiments how long the code will take to run. Alternatively, set `verbose` to 2 in early test runs, but then set it to 0 in your final run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o71nGK09Fw3Z",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "PN-T-s-5c8tp",
    "outputId": "ac578677-aec5-49b0-e109-a5320272b601",
    "tags": [
     "S"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 125us/step\n",
      "Test loss: 0.7215502796649933\n",
      "Test accuracy: 0.7787\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI4lBCCFEP/8fD75RnIJ8k5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = train(model, epochs=30, opt=keras.optimizers.Adam(), verbose=0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_accuracy(history):\n",
    "  plt.plot(history[0], history[1], label='training')\n",
    "  plt.plot(history[0], history[2], label='validation')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPBYoj7rwlKg",
    "tags": [
     "S"
    ]
   },
   "source": [
    "Training longer is likely to be only marginally beneficial if at all. While the training accuracy is still on the rise and is likely to keep improving, validation accuracy is rather flat, indicating that the classifier is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxdW8dV4uoCq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "### Problem 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A97WOgV8uweq",
    "tags": [
     "HST"
    ]
   },
   "source": [
    "Suggest at least three different ways to improve the performance of the classifier defined in this Part. For each way, explain why that would help. This is an open-ended question, and answers may vary. Do _not_ implement your suggestions, and do _not_ refer to techniques we have not covered in class (such as batch normalization or other techniques you may have heard of).\n",
    "\n",
    "If you suggest more than three ways, we will grade you for the best ones. However, we _will_ deduct points for patently wrong statements in any of your suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12XxnmJwnYzN",
    "tags": [
     "ST"
    ]
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u90B6CHGnaOx",
    "tags": [
     "S"
    ]
   },
   "source": [
    "+ Use 50000 images for training, and check validation performance by cross-validation, rather than by setting 10000 of the 50000 images aside for this purpose as done above. This will increase the size of the training set, somewhat improving generalization.\n",
    "+ Use data augmentation. This results in the training algorithm effectively seeing more data, and the ratio of training set size to number of parameters improves.\n",
    "+ Add more layers to the network, to give it more expressive power. AlexNet works better on harder problems, and has more layers. This may require adding data to the training set.\n",
    "+ Train the network multiple times with different initializers, and pick the network that yields the highest validation accuracy. Training converges to local minima of the risk function, and different minima may perform differently.\n",
    "+ Train several networks with different initializers and/or on different subsets of the data, and combine their predictions through a majority vote. This combination will smooth out overfitting errors and lead to better generalization.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "solution05.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
